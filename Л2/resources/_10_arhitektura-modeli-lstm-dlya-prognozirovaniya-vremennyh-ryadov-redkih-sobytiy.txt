
АРХИТЕКТУРА МОДЕЛИ LSTM ДЛЯ ПРОГНОЗИРОВАНИЯ ВРЕМЕННЫХ РЯДОВ РЕДКИХ СОБЫТИЙ

  Анализ временных рядов относится к анализу изменения тренда данных за определенный период времени. Анализ временных рядов имеет множество применений, от прогнозирования будущей стоимости товара на основе его прошлых значений до более сложных экономических и информационных явлений [1].
  Долгая краткосрочная память (LSTM) – особая разновидность архитектуры рекуррентных нейронных сетей (РНС), способная к обучению долговременным зависимостям. Они могут решать ряд разнообразных задач и в настоящее время обширно используются.
  LTSM сети были представлены и разработаны З. Хохрайтер и Ю. Шмидхубером в 1997 году специально, для того, чтобы избегать проблем долговременной зависимости. Их специализация – запоминание информации в течение длительных периодов времени, поэтому они не нуждаются в долгом обучении.
  Все рекуррентные нейронные сети имеют форму цепочки повторяющихся модулей нейронной сети. В стандартных РНС этот повторяющийся модуль имеет простую структуру, например, один слой tanh[2].
  На приведенной выше диаграмме каждая линия является вектором. Розовые круги обозначают поточечные операции, например, суммирование векторов. Желтые ячейки – это слои нейронной сети. Совмещение линий – это объединение векторов, а знак разветвления – копирование вектора с последующим хранением в разных местах.

  Ключевым	понятием	LSTM	является	состояние	ячейки	(горизонтальная	линия, проходящая через верхнюю часть диаграммы).


Рис. 1. Структура обычной РНС


Рис. 2. Структура LTSM

  Состояние ячейки похоже на конвейерную линию. Оно проходит через всю цепочку, подвергаясь незначительным линейным преобразованиям.
  В LSTM уменьшает или увеличивает количество информации в состоянии ячейки, в зависимости от потребностей. Для этого используются гейты – структуры, которые тщательно настраиваются.
  Гейт – это «ворота», пропускающие или не пропускающие информацию. Гейты состоят из сигмовидного слоя нейронной сети и операции поточечного умножения.
  Но не все LSTM одинаковы. Существует много вариаций LSTM. Отличия между ними незначительны, но о некоторых из них стоит упомянуть.
Одна из популярных вариаций LSTM, характеризуется добавлением так называемых
“смотровых глазков”. С их помощью слои фильтров могут видеть состояние ячейки.


Рис. 3. Структура LTSM с добавлением “смотровых глазков”

  Другие модификации включают объединенные фильтры “забывания” и входные фильтры. В этом случае решения, какую информацию следует забыть, а какую запомнить, принимаются не отдельно, а совместно. Мы забываем какую-либо информацию только тогда, когда необходимо записать что-то на ее место. Мы добавляем новую информацию с состояние ячейки только тогда, когда забываем старую.

Рис. 4. Структура LTSM с фильтрами “забывания”

  Немного больше отличаются от стандартных LSTM управляемые рекуррентные нейроны. В ней фильтры «забывания» и входа объединяют в один фильтр «обновления». Кроме того, состояние ячейки объединяется со скрытым состоянием, есть и другие небольшие изменения. Построенная в результате модель проще, чем стандартная LSTM.

Рис. 5. Структура LTSM с управлемыми рекуррентными нейронами

  Это лишь несколько самых примечательных вариаций LSTM. Существует множество других модификаций.
  LSTM – большой шаг в развитии РНС. При этом возникает вопрос, каким будет следующий большой шаг. По общему мнению исследователей, следующий шаг заключается в использовании механизма внимания. Идея состоит в следующем: каждый шаг РНС берет данные из более крупного хранилища информации. Например, если мы используем РНС для генерации подписи к изображению, то такая РНС может рассматривать изображение по частям и на основании каждой части генерировать отдельные слова. Последние несколько лет – время расцвета рекуррентных нейронных сетей, и следующие годы обещают принести еще большие плоды.[3]












