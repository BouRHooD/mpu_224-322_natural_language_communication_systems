БОЛЬШИЕ ДАННЫЕ И ПРИЛОЖЕНИЯ / BIG DATA AND APPLICATIONS

Введение
Современные достижения в области информационных тех- нологий и их широкое распространение в различных сферах деятельности привело к взрывному росту обрабатываемых данных и возникновению понятия «больших данных» [1, 2]. Наряду с ростом объема данных, происходит и рост сложности задач, решаемых с помощью информационных технологий, что ведет не только к росту масштаба вычислительных ин- фраструктур, но и к усложнению их устройства. Так, например, достижения в области технологий виртуализации и значи- тельный рост объемов вычислительных ресурсов привели к возникновению нового направления исследований в области технологий построения и применения вычислительных ин- фраструктур – облачных вычислений [3, 4].
Одной из основных идей облачных вычислений является вве- дение дополнительного уровня абстракции над физическими ресурсами вычислительной инфраструктуры в виде виртуаль- ных машин (ВМ). При этом ВМ могут как являться конечным продуктом облачной инфраструктуры, так и служить базой для построения сложных многокомпонентных информаци- онно-вычислительных систем, которые сами могут являть- ся облачными сервисами более высокого уровня. Очевидно, что подобное усложнение структуры вычислительных ин- фраструктур и сервисов приводит и к усложнению процесса управления ими.
Одной из актуальных проблем, возникающих при управлении облачными инфраструктурами, является обработка возни- кающих нештатных ситуаций и сбоев в работе системы [5-7]. Так, задачу поиска первоисточников возникающих неполадок значительно усложняет распределенная архитектура совре- менных систем с большим количеством взаимосвязей между их компонентами. Для ее решения, как правило, используется анализ данных журналов событий и различных показателей функционирования системы, получаемых с помощью систем мониторинга. Актуальность данной проблемы также под- тверждается и большим количеством исследований, направ- ленных на изучение методов извлечения полезной информа- ции из получаемых и, как правило, слабоструктурированных данных [8-11].
Другой актуальной проблемой является падение скорости извлечения данных из-за быстрого увеличения объемов по- лучаемых данных. При возникновении нештатных ситуаций, скорость извлечения данных о функционировании системы напрямую влияет на скорость исправления возникающих сбо- ев и, соответственно, на общую стабильность работы системы. Для решения описанных проблем и задач разрабатываются программные продукты и комплексы, которые позволяют ор- ганизовать сбор и хранение данных, а также предоставляют инструменты для их анализа. В данной работе рассматрива- ется подобное техническое решение и аспекты его внедрения для анализа журналов событий облачной инфраструктуры Объединенного института ядерных исследований (ОИЯИ).
Облачная инфраструктура ОИЯИ
ОИЯИ принимает участие в большом количестве различных научных и образовательных проектов и экспериментов, кото-

рые нуждаются в надежной и масштабируемой информацион- ной вычислительной инфраструктуре. На базе Лаборатории информационных технологий (ЛИТ) ОИЯИ была развернута облачная инфраструктура, базирующаяся на модели инфра- структура как сервис (англ. Infrastructure as a Service, IaaS). По данной модели конечным продуктом облачного сервиса являются виртуальные машины – виртуальные серверы с не- обходимой вычислительной мощностью. Облако ОИЯИ [12] построено на основе проекта с открытым исходным кодом OpenNebula, в состав которого входят средства для разверты- вания виртуального окружения, управления хранилищами данных, контроля доступа и мониторинга.
Сервис используется как для предоставления персональных ВМ индивидуальным пользователям, так и как основа для ряда многоузловых информационно-вычислительных комплексов, например, таких, как виртуальный вычислительный кластер HTCondor и сервис интерактивных вычислений JupyterHub. Так- же облако ОИЯИ является частью интегрированной облачной инфраструктуры стран-участниц ОИЯИ [13, 14], состоящей из отдельных облачных инфраструктур ее участников. Подобная интеграция дает возможность использования всех отдельных облачных вычислительных инфраструктур участников через единый интерфейс запуска вычислительных задач.
Облако ОИЯИ активно развивается. В 2015 году вычислитель- ный ресурс облака ОИЯИ уже насчитывал 200 ядер ЦП и 400 ГБ оперативной памяти. К 2016 году цифры выросли до значений 330 ядер ЦП и 840 ГБ оперативной памяти. По состоянию на 2019 год, количество ядер ЦП возросло до 1564, а объем опера- тивной памяти до 8.54 ТБ. На текущий момент вычислитель- ные мощности составляют 5000 ядер ЦП и 56 ТБ ОЗУ. На дан- ный момент облако состоит из 205 серверов, которые условно можно отнести к трем типам: управляющие, вычислительные и узлы хранения. 184 сервера выделены под виртуальные ма- шины пользователей. Высокая динамика роста вычислитель- ных ресурсов влечет за собой увеличение числа виртуальных серверов, что, в свою очередь, значительно увеличивает по- тенциальный объем обрабатываемых данных. Хотя структура облака и является относительно простой на физическом уров- не, на уровне использующих ее сервисов структура усложняет- ся и приобретает динамический характер, в частности, за счет их масштабируемости. Упрощенная схема облачной инфра- структуры ОИЯИ представлена на рисунке 1.



Р и с. 1. Упрощенная схема облачной инфраструктуры ОИЯИ F i g. 1. Simplified diagram of the JINR cloud infrastructure




Задача сбора и анализа журналов событий становится все бо- лее и более сложной из-за роста масштабов облака ОИЯИ, а также внедрения новых сервисов, построенных на его основе, и интеграции со сторонними вычислительными системами.
Требования к системе
Для обеспечения надежности и доступности сервисов требу- ется оперативно реагировать на отклоняющиеся от нормы события, возникающие в кластере облачной инфраструкту- ры. Критические ошибки и предупреждения могут повлечь за собой сбой функционирования системы, что может привести к потере пользовательских данных, данных экспериментов. Не исключены и угрозы информационной безопасности. Для выявления всех аномальных явлений необходимо проводить непрерывный системный сбор и анализ лог-файлов инфра- структуры, что невозможно без использования сторонних специализированных инструментов. Для решения этой зада- чи необходимо внедрить в облако ОИЯИ специализированную систему сбора и анализа лог-файлов.
Основные требования к системе были сформулированы исхо- дя из описанной выше специфики облачной инфраструктуры ОИЯИ, они включают:
• Возможность интеграции с системой единого входа ОИЯИ (Single Sign-On, SSO).
• Возможность масштабирования системы при увеличении нагрузки.
• Способность обрабатывать слабоструктурированные данные в различных форматах, в частности - файлы си- стемных журналов событий.
• Предоставление механизма защиты передаваемых по сети данных в процессе их сбора.
• Наличие графического пользовательского интерфейса для облегчения взаимодействия операторов с системой.
• Возможность установки собственного экземпляра систе- мы на собственных вычислительных мощностях.
• Распространение по лицензии с открытым исходным ко- дом для обеспечения возможности свободной доработки системы, исходя из локальных особенностей.
Реализация системы
Основные компоненты
При реализации системы была задействована совокупность инструментов по сбору, анализу, агрегации и фильтрации дан- ных, также известная под названием ELK Stack:
• Elasticsearch – распределенная система поиска и аналити- ки, которая позволяет хранить, анализировать и искать информацию из больших объемов данных. В своем ядре содержит Lucene – библиотеку для полнотекстового высо- копроизводительного поиска.
• Logstash – конвейер обработки данных, который позво- ляет фильтровать и преобразовывать принимаемые дан- ные и передавать полученные результаты одному или нескольким получателям.
• Kibana – веб-интерфейс для визуализации данных.

Elasticsearch применяется для решения большого спектра задач, начиная от обычного поиска слов в тексте [15], закан- чивая сбором и анализом метрик производительности [16], геоданных [17], различных системных данных [18] и даже ма- шинным обучением [19]. Рассматриваемый стек технологий хорошо себя зарекомендовал как в индустрии, так и в научных и образовательных организациях [20-25].
Elasticsearch Stack – программный продукт с поддержкой кла- стерной архитектуры, что позволит горизонтально масштаби- ровать сервис при быстро растущем облаке.
ELK Stack является системой с открытым исходным кодом под лицензией Apache 2.0, что позволяет бесплатно использовать его продукты, в то же время у компании Elastic есть коммер- ческий продукт X-Pack под лицензией Elastic License, что суще- ственно усложняет работу с продуктами Elastic. Часть функ- ционала X-Pack доступна по пробной версии, но основные и довольно важные функциональные возможности распростра- няются только под коммерческой лицензией:
• Index Lifecycle Management – позволяет управлять состоя- нием индексов (индекс - специализированная структура и механизм управления данными в Elasticsearch).
• Infrastructure and Logs UI – плагин для визуализации, фильтрации и просмотра лог-файлов.
• Kibana multi-tenancy – инструменты для разграничения прав доступа пользователей к различным объектам си- стемы (индексам, панелям визуализации данных и т.д.), необходимые для реализации многопользовательского использования системы.
Весной 2019 года Amazon выпустил Open Distro for Elasticsearch1 под свободной лицензией Apache 2.0, который включает в себя свободные реализации многих возможностей из X-Pack. Во-первых, набор функций безопасности:
• Node-to-node encryption – шифрование трафика между уз- лами Elasticsearch кластера.
• HTTP basic authentication – метод аутентификации, кото- рый включает в себя имя пользователя и пароль как часть HTTP-запроса.
• Role-based access control – управление доступом на основе ролей.
• Kibana multi-tenancy – инструменты разграничения прав доступа.
• Alerting – система для отправки уведомлений.

Во-вторых, SQL, что предоставляет возможность писать запро- сы на языке SQL, а не на предметно-ориентированном языке запросов Elasticsearch (DLS).
В-третьих, Performance Analyzer – REST API, позволяющий полу- чать различные метрики производительности кластера.
В-четвертых, Index Management – позволяет управлять индек- сами.

Используемый в описываемых работах дистрибутив Open Distro for Elasticsearch является полностью свободным и от- крытым программным решением, основная цель которого – обеспечение дальнейшего развития инновационных про- граммных решений с открытым исходным кодом и свободное



1 Open Distro for Elasticsearch [Электронный ресурс]. URL: https://opendistro.github.io/for-elasticsearch (дата обращения: 02.03.2021).




распространение полнофункционального дистрибутива си- стемы.

Интеграция с SSO ОИЯИ
Одним из важнейших критериев выбора системы являлась возможность ее интеграции с системой SSO ОИЯИ, которая является собственной реализацией популярного протокола аутентификации OAuth22. Хотя выбранный стек технологий и не предоставляет прямой поддержки протокола OAuth2, он имеет механизм подключения внешних систем аутентифика- ции через проксирующий сервер. Функциями прокси-сервера являются переадресация пользователя на страницу входа в системе SSO ОИЯИ и передача необходимых пользовательских

данных сервисам ELK.
Для реализации описанного механизма был разработан соб- ственный прокси-сервер на основе проекта OAuth2 Proxy3, осуществляющий перенаправление пользовательских запро- сов на вход в систему SSO ОИЯИ и получения от нее пользо- вательских данных в случае успешной аутентификации. Для формирования корректного HTTP-запроса на вход в Kibana (из получаемых OAuth2 Proxy данных о пользователе) был исполь- зован широко известный веб-сервер Nginx. Также Nginx был использован и для установления безопасных соединений меж- ду веб-браузером пользователя, системой ELK и SSO ОИЯИ. Схе- ма работы этого механизма проиллюстрирована на рисунке 2.





Р и с. 2. Схема интеграции технологии SSO в систему анализа данных F i g. 2. Scheme for integrating SSO technology into a data analysis system

Защита сетевых соединений
Одним из требований к разрабатываемой системе было предо- ставление механизма защиты передаваемых по сети данных в процессе их сбора. В системе можно выделить три категории сетевых соединений, которые необходимо защитить:
• связь веб-браузера клиента с веб-интерфейсом Kibana;
• связи между компонентами ELK, находящихся на разных сетевых узлах;
• связи между узлами, с которых осуществляется сбор данных, и подсистемами ELK.

Защита первой категории соединений была обеспечена за счет шифрования трафика веб-сервером Nginx при реализации интеграции системы с SSO ОИЯИ.
Для обеспечения шифрования всех внутренних соединений, попадающих в оставшиеся две категории, в ELK предусмотрен собственный механизм, для реализации которого требуется наличие удостоверяющего центра (УЦ). Задачей УЦ является подтверждение подлинности ключей шифрования с помощью сертификатов электронной подписи.
Подобный механизм защиты имеет широкую область приме-

нения, и в крупных вычислительных инфраструктурах, подоб- ных облачной инфраструктуре ОИЯИ, единый УЦ может быть использован в разных компонентах инфраструктуры. В част- ности, в рассматриваемой инфраструктуре уже был ранее соз- дан УЦ для защиты соединений в системе управления конфи- гурациями Puppet4, использующейся для конфигурирования облачных узлов. Таким образом, уже на этапе первоначальной настройки новые узлы получают собственные сертификаты, подписанные локальным УЦ, которые впоследствии использу- ются для шифрования всех внутренних соединений. Подобная интеграция позволила избежать дублирования функционала для решения схожих задач в разных частях инфраструктуры.

Применение Logstash
Одним из примеров, иллюстрирующих динамическое измене- ние конфигурации облачной инфраструктуры, является ме- ханизм обеспечения отказоустойчивости управляющих узлов облака ОИЯИ. Он основан на консенсусном алгоритме Raft5, который гарантирует согласованность серверов относительно друг друга. Это достигается тем, что в кластере из 3 управляю- щих узлов, путем голосования, выбирается специальный узел,



2 Hardt D. The OAuth 2.0 Authorization Framework. RFC 6749. October 2012. DOI: 10.17487/RFC6749 [Электронный ресурс]. URL: https://www.rfc-editor.org/info/ rfc6749 (дата обращения: 02.03.2021).
3 Speed J. OAuth2 Proxy [Электронный ресурс]. URL: https://github.com/oauth2-proxy/oauth2-proxy (дата обращения: 02.03.2021).
4 Puppet: Powerful infrastructure automation and delivery [Электронный ресурс]. URL: https://puppet.com (дата обращения: 02.03.2021).
5 OpenNebula [Электронный ресурс]. URL: https://docs.opennebula.io/5.12/advanced_components/ha/frontend_ha_setup.html#raft-overview (дата обращения: 02.03.2021).




называемый «лидером», который будет обеспечивать функци- ональность системы. Лидер периодически посылает сигналы остальным узлам, которые называются «последователи», чтобы сохранить свой статус лидерства. Если управляющему узлу кла- стера не удалось отправить сигнал, то выбираются кандидаты и начинаются новые выборы лидера. При каждой модификации системы, прежде чем сделать запись о смене состояния системы в базе данных (БД), лидер записывает в БД последовательность операций, которые необходимо выполнить для смены состо- яния системы, и реплицирует эти записи на последователей. Данная операция увеличивает задержку выполнения операций при работе с БД, но гарантирует сохранение целостности состо- яния системы и обеспечивает сохранение работоспособности кластера при выходе из строя лидирующего узла.
Из описания алгоритма Raft видно, что управляющие узлы в каждый отдельно взятый момент времени будут иметь раз- личные роли, которые со временем могут измениться (напри- мер, в случае возникновения сбоя в системе). При этом каж- дый узел ведет собственные системные журналы событий, но из-за возможной смены ролей узлов отдельные части содер- жимого одного и того же лог-файла на отдельно взятом узле могут соответствовать разным ролям, что затрудняет анализ журналов событий.
Алгоритм Raft предусматривает наличие у лидера дополни- тельного «плавающего» ip-адреса, который при выходе из строя текущего лидера переходит к новому. Так как у каждого из трех узлов всегда имеется свой собственный ip-адрес, плава- ющий ip-адрес можно использовать как индикатор наличия у узла роли «лидер». Таким образом, упростить анализ журнала событий можно, организовав запись системных журналов не в три индекса Elasticsearch, а в четыре, соответствующих трем постоянным и одному плавающему ip-адресу. В данной органи- зации четвертый индекс всегда будет содержать только записи журналов (что проиллюстрировано на рисунке 3), соответству- ющие лидирующему узлу, и при этом сохранится возможность независимого просмотра журналов всех трех узлов.

Для реализации описанной выше задачи был использован функционал подсистемы Logstash. С помощью встроенных фильтров Logstash извлекает ip-адреса управляющих узлов, сравнивает их с заданным шаблоном и отправляет данные в соответствующие индексы, что позволяет просматривать за- писи журналов независимо как со всех трех узлов, так и записи только лидирующего узла. Данная реализация проиллюстри- рована на рисунке 4.


Р и с. 4. Схема пополнения индексов данными, содержащими записи журналов событий управляющих узлов
F i g. 4. Scheme for replenishing indexes with data containing records of the event logs of control node

Заключение













Р и с. 3. Схема формирования единого журнала лидирующего узла. Цветом выделены блоки записей журнала событий, поступающие с соответствующих серверов.
F i g. 3. The formation scheme of a single journal of the leading node. The color shows the blocks of event log records coming from the corresponding servers.

Анализ системных журналов является одной из важнейших задач для поддержания стабильной работы любой сложной программной системы, и с ростом масштабов и сложности со- временных информационно-вычислительных систем данная задача становится все более трудновыполнимой. В данной ра- боте был описан опыт внедрения стека технологий ELK в ка- честве системы сбора и анализа системных журналов событий облачной инфраструктуры ОИЯИ. Данный опыт показывает, что, используя данный стек технологий, можно довольно бы- стро и достаточно просто построить систему сбора и анализа системных журналов событий для осуществления централи- зованного мониторинга IT-инфраструктуры. В дальнейшем планируется расширить область применения системы на все




облачные сервисы ОИЯИ, а также оценить возможность её применения и для сбора журналов работы пользовательских вычислительных задач, выполняемых на кластере HTCondor. Использование ELK в качестве системы анализа системных журналов событий – лишь частный случай в текущей прак- тике, данный инструмент может быть востребован и в других исследовательских проектах ОИЯИ.









