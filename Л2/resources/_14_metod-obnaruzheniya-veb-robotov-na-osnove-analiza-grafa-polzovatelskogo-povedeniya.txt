
Метод обнаружения веб-роботов  
на основе анализа графа пользовательского поведения 

 
  Посетителей сегодняшних веб-ресурсов можно условно разделить на две категории: легитимные пользователи, совершающие действия при помощи веб-браузеров и мобильных приложений, и веб-роботы, выполняющие на сайте автоматизированные действия [1]. Веб-роботы могут выступать в роли индексаторов ресурса, проверять ссылки и работоспособность функционала, но могут и нести различные автоматизированные угрозы – от кражи информации до совершения мошеннических действий и манипуляций с целью получения преимущества над обычными пользователями [2]. 
   Отчеты компаний, которые занимаются мониторингом Интернета, показывают, что до 50 % трафика на сайте приходит от веб-робо- тов [3]. Различие статистических параметров поведения пользователей веб-ресурсов и веброботов можно использовать также для улучшения системы кэширования и настройки систем управления статистикой для исключения веб-роботов из различных маркетинговых отчетов [4]. 
   Типичный веб-ресурс – это ориентированный граф, узлами которого являются веб-страницы с информацией (HTML-страницы, документы, файлы, изображения, скрипты), а ребро проводится из узла, где есть гиперссылка в узел, на который она ведет. Ссылкой также могут являться вложение ресурса (например, изображения) и переход, выполняемый из Java- Script-сценария [5]. 

Программные продук
   Знание о структуре веб-ресурса и данные о поведении легитимных пользователей на нем можно использовать для формирования модели поведения. Отличие поведения пользователя от данной модели позволяет сделать вывод об автоматизации его посещений и о других целях перемещения по сайту. Данные факты могут быть использованы для обнаружения веб-роботов и уточнения классических синтаксических и аналитических методов классификации пользователей.  
   Обнаружение веб-роботов. Обнаружение происходит на основе анализа данных о пользователе. Часто такими данными являются обычные логи веб-сервера, но это могут быть дампы трафика или данные уровня приложений [6]. 
   Логи веб-сервера – это наборы строк, содержащих следующие данные о каждом запросе к веб-ресурсу: дата, путь до запрашиваемого узла, код ответа, страница, с которой совершен переход, браузер пользователя, IP-адрес источника, уникальный идентификатор сессии (если настроен). 
   Для каждой пользовательской сессии рассчитываются 	уникальные 	характеристики, описывающие поведение данного пользователя на веб-ресурсе. На их основе каждая сессия классифицируется на легитимную или роботизированную. 
   Классические методы обнаружения сегодня используют данные пользовательских запросов и логов без привязки к реальной структуре и контенту, расположенному на веб-ресур- се [7]. В основном исследователи применяют различные методы классификации или кластеризации на основе информации, полученной из веб-логов. Такие подходы позволяют добиваться точности обнаружения вплоть до 0.9 [8, 9], однако результаты очень зависят от набора данных и наличия в нем сложных веб-роботов, скрывающих свое присутствие [10]. 
   Построение графа веб-ресурса. Отдельной задачей является получение графа веб-ресурса (рис. 1). Для этого могут использоваться как внешние системы (краулер, совершающий обход всех страниц веб-ресурса), так и внутренний подход (генерация графа связности стра-
                              (1) ниц через расширенные возможности фреймворка, 	на 	котором 	основан 	веб-ресурс). Связность также можно генерировать на ос-
нове пользовательских сессий, но такой подход      (2) приводит к ошибкам и ложным срабатываниям за счет устаревания данных в логах, нерелевантных запросов от веб-роботов, скрывающих 

Программные продук
   – меры центральности (Closeness centrality, Betweenness centrality, Harmonic centrality, eigencentrality); 
   – значения алгоритма HITS (ранги авторов и посредников) [11]; – PageRank [12]: 
           R v?( )) R u'( ) = cv B?? u Nv + cE u( ).      (3) 
   Характеристики сессий. Каждая сессия представляет собой набор запросов к веб-ресурсу от одного источника за определенный временной интервал. Каждый запрос направлен к определенному узлу графа, что позволяет оценивать изменение характеристик каждой из вершин графа, а также вычислять комбинированные показатели. 
   В результате каждую из сессий можно характеризовать следующими типами признаков, на основе которых использовать классификацию: 
   – среднее значение каждой из характеристик каждого узла; 
   – среднеквадратические отклонения характеристик узлов; 
   – распределение значений по каждой характеристике; 
   – дополнительные характеристики переходов между узлами. 
   Изучение распределений значений для различных характеристик позволяет утверждать, что шаблоны поведения легитимных пользователей и веб-роботов отличаются (рис. 3). 
   К дополнительным характеристикам переходов относятся технические особенности перемещения по графу: 
   – количество переходов между страницами, не связанными ссылкой; 
   – количество возвратов на предыдущую страницу. 
   В данном исследовании не рассматривается временной контекст каждого из запросов, однако стоит отметить, что учет временных интервалов между разными типами запросов может принести дополнительные знания о том, как быстро пользователи принимают те или иные решения о возврате на предыдущую страницу или о переходе на главную страницу. 
   Сравнительный анализ. В исследовании использовался архив трафика к веб-ресурсу за один месяц. Архив содержит HTTP-запросы к сайту от шестидесяти тысяч источников за рассматриваемый период. Веб-ресурс использует специальное ПО для идентификации сессий, что позволяет однозначно идентифицировать 
Closeness centrality
14 12
10 8
6 4
2
0 0	0,1	0,2	0,3
	Веб-роботы	Пользователи
 
 
а) 
 
Betweenness centrality
70
60
50
40
30
20
10
0
-0,1	0,4	0,9	1,4 Веб-роботы	Пользователи
 
 
б) 
Рис. 3. Разница значений характеристик  
а) Closeness centrality и б) Betweenness centrality для людей и веб-роботов  
 
    Fig. 3. The difference in the values  of the characteristics: a) Closeness centrality  
and б) Betweenness centrality for people  and web robots 
связанные сессии легитимных пользователей без использования нечетких алгоритмов идентификации. 
  С помощью разработанного ПО в полуавтоматическом режиме производилась предварительная классификация сессий с использованием как однозначных признаков веб-робота (запросы к файлам-ловушкам, известные адреса источников, известные значения UserAgent), так и дополнительных параметров, оцениваемых человеком (повторяемость запросов, аномалии поведения, исполнение JavaScript, географическая привязка источника и другие). 
   При помощи ExtraTreesClassifier оценивались значимости всех непосредственных и усредненных графовых признаков для уменьшения признакового пространства (см. таб- Программные продук лицу). Дополнительно производилась оценка корреляции признаков. 
 
Наиболее значимые признаки The most significant features 
 
 
Признак 
Важность 
clustering_std 
0.060268 
harmonicclosnesscentrality_std 
0.054645 
eigencentrality_std 
0.054076 
degree_std 
0.053189 
hub_std 
0.051802 
outdegree_std 
0.051132 
closnesscentrality_std 
0.050760 
authority_std 
0.045223 
indegree_std 
0.043336 
clustering_avg 
0.043311 
 
   Для классификации использовались несколько разных моделей: Gradient Boosting, XGboost, Multilayer perceptron. Набор данных был разделен на тренировочный и тестовый. Тренировочный использовался для сравнения моделей классификации и подбора гиперпараметров моделей с использованием кроссвалидации с разделением на 10 блоков. Итоговая оценка производилась на тестовом наборе данных. 
   Оптимизация гиперпараметров производилась при помощи Grid Search с минимизацией значения площади под кривой ошибок (рис. 4). 
    Все рассматриваемые модели после оптимизации гиперпараметров показали приемлемые результаты обнаружения. Модель XGboost была наиболее точна с F1-мерой, равной 0.96. 
 
 
ROC-кривая
1
0,8
0,6
0,4
0,2
0
	0	0,2	0,4	0,6	0,8	1
False Positive Rate
 
 
Рис. 4. ROC-кривая Fig. 4. ROC curve 
 
 
Выводы 
 
  В статье предложен метод обнаружения веб-роботов на основе анализа графа пользовательского поведения. За счет анализа связности страниц веб-ресурса и расчета характеристик графа веб-ресурса удалось добиться улучшения точности и полноты обнаружения веб-роботов. Данные характеристики могут быть скомбинированы с классическими методами и приводить к улучшению показателей обнаружения веб-роботов. Были использованы несколько методов классификации, произведены подбор гиперпараметров, а также перекрестная проверка результатов обнаружения. В итоге достигнута F1-мера обнаружения веб-роботов, равная 0.96, что превышает существующие показатели методов, основанных на синтаксическом и аналитическом обнаружении. 

   Работа выполнена в рамках гранта РФФИ № 17-07-00700-а «Методы формальной и функциональной верификации вычислительных процессов, основанные на знаниях и графоаналитических моделях». 
 
 
